
#Рекуррентная сеть Хэмминга

#3 слоя нейронов; в каждом слое по 10 нейронов; число входов = 49; выходы принимают значения {-1, 1}

#Алгоритм обучения (шаги):
#формируется матрица эталонных образов 10х49 (получается, заполняется эталонными примерами)
#рассчитывается матрица весовых коэффициентов первого слоя нейронов (на основе эталонной матрицы)
#задаются значения синапсов обратных связей матрицы второго слоя нейронов 10х10 (по диагонали 1, а так = -(1 / (p - 1)) + epsilon), где epsilon - случайная очень маленькая величина
#задаётся линейная функция активации (y, y >= 0; 0, y < 0)
#задаётся максимально допустимое значение нормы разности выходных векторов на двух последовательных итерациях е = 0.1

#Алгоритм применения (шаги):
#подаётся зашумлённый пример
#рассчитываются состояния и выходные значения нейронов первого слоя, состояния => y_i = 1 - (1/2 * (49 - sum_1_to_49(x^t_i * x_i))), а выходы - состояния через функцию активации
#выходы нейронов второго слоя = выходы первого слоя
#для каждой из s (пусть s = max_iter = 100) итераций рассчитываются новые выходы второго слоя => y_i(s) = f(y_i * (s - 1) + sum_j_!=_i(w^m_ij * y_j * (s - 1)))
#предыдущий шаг повторяется, пока ||y(s) - y(s - 1)|| <= e
#на выходе этих всех состояний нужно получить вектор из 10 элементов, где все нулевые, а ненулевой только тот, который является индексом наиболее подходящего примера, на который похож восстановленный шумный пример
#зачем нужен третий слой - непонятно

import numpy as np

def euclid_norm(y_new: list[float], y_previous: list[float]) -> float:
    """
    Вспомогательная функция которая считает евклидову норму между векторами
    Args: y_new и y_previous - 2 вектора
    Return: норма между векторами
    """
    norm_new = np.linalg.norm(y_new)
    norm_previous = np.linalg.norm(y_previous)
    return np.linalg.norm([norm_new], [norm_previous])

class Hamming_Network:

    def __init__(self) -> None:
        """
        Функция, которая инициализирует слои сети
        Args: отсутствуют
        Return: отсутствует
        """
        self.w1 = [[0 for _ in range(49)] for _ in range(10)]
        self.w2 = [[0 for _ in range(10)] for _ in range(10)]
        self.w3 = 0

    def init_basic_mat(self, examples: list[list[list[int]]]) -> None:
        """
        Функция, которая заполняет матрицы первого и второго слоя нейронов
        Args: examples - список примеров, каждый их которых - матрица 7х7
        Return: отсутствует
        """
        #заполняем матрицу первого слоя
        for example in examples:
            #переводим пример из 7х7 в 1х49
            example = np.array(example).reshape(-1,).tolist()
            #в методичке предлагается делать так, а на сайте, с которого была взята информация, второй вариант
            for i in range(len(self.w1)):
                self.w1[i] = example
                #print(self.w1[i])
            """ for i in range(len(self.w1)):
                for j in range(len(self.w1[i])):
                    self.w1[i][j] = 1/2 * example[j]
                #print(self.w1[i]) """
        #заполняем матрицу второго слоя
        for i in range(len(self.w2)):
            for j in range(len(self.w2[i])):
                if i == j:
                    self.w2[i][j] = 1
                else:
                    #в методичке предлагается делать так, а на сайте, с которого была взята информация, второй вариант (хотя тут второй вариант какой-то непонятный)
                    #создаём случайное малое число от 0 до 1 (по умолчанию создаётся матрица 1х1, так что указываем сам элемент [0][0])
                    epsilon = np.random.rand(1, 1)[0][0]
                    self.w2[i][j] = -(1 + 9 * epsilon) / 9
                    """ self.w2[i][j] = -epsilon """
            #print(self.w2[i])

    def fun_activation(self, y: float) -> float:
        """
        Функция, которая реализует линейную пороговую функцию активации
        Args: y - входное значение
        Return: значение функции активации
        """
        #в методичке предлагается делать так, а на сайте, с которого была взята информация, второй вариант
        return y if y >= 0 else 0
        """ t = 49/2
        if y < 0: return 0
        elif 0 < y <= t: return y
        else: return t """

    def calculate_outputs_w1(self, examples_with_noise: list[list[list[int]]]) -> list[float]:
        """
        Функция, которая считает состояния и выходы первого слоя нейронов
        Args: examples_with_noises - список шумных примеров в виде матриц 7х7
        Return: список выходов нейронов
        """
        #рассчитываем состояния
        states = []
        #тут чё-то ругалось на for i in range(len(examples_with_noise)): example = np.array(examples_with_noise[i]).reshape(-1,).tolist()
        for i, example in enumerate(range(len(examples_with_noise))):
            #в методичке используется синтаксис sum_1_to_49(x^t_i * x_i), где t (скорее всего) от 1 до p (т.е. 10), но сам пример имеет длину 49, так что костылим
            example = np.array(example).reshape(-1,).tolist()
            tmp_avg_sum_w = sum(self.w2[i]) / len(self.w2[i])
            tmp_sum = 0
            for j in range(len(example)):
                tmp_sum += example[j] * tmp_avg_sum_w
            y = 1 - ((1/2 * (49 - tmp_sum)) / 49)
            states.append(y)
        #рассчитываем выходы 
        results = []
        for i in range(len(states)):
            results.append(self.fun_activation(states[i]))
        #print(results)
        return results

    def calculate_outputs_w2(self, outputs_w1: list[float], examples_with_noise: list[list[int]], max_iter=100, e=0.1) -> list[int]:
        """
        Функция, которая считает выходы второго слоя нейронов
        Args: outputs_w1 - выходы первого слоя нейронов; examples_with_noise - список шумных примеров; max_iter - s - максимальное кол-во итераций; e - норма разности векторов
        Return: список выходов нейронов
        """
        result = []
        for i, example in enumerate(examples_with_noise):
            #y_previous = np.array(example).reshape(-1,).tolist()
            y_previous = outputs_w1
            tmp_avg_sum_w = sum(self.w2[i]) / len(self.w2[i])
            for _ in range(max_iter):
                tmp_sum = 0
                for j in range(len([y_previous])):
                    #после первой итерации y_previous перестаёт быть списком и становится числом
                    try:
                        tmp_sum += y_previous[j] * tmp_avg_sum_w
                    except:
                        tmp_sum = y_previous * tmp_avg_sum_w
                y_new = self.fun_activation(tmp_sum)
                #y_new оборачивается в массив чтобы не было ошибки object numpy.float64 has no len()
                if euclid_norm([y_new], y_previous) <= e:
                    result.append(y_new)
                    break
                #конвертируем y_new в список, чтобы не было проблем с атрибутом len()
                y_previous = y_new.tolist()
            #проверка (если закончились итерации, а в массиве на 1 меньше элементов - добавляем)
            if (len(result) < i + 1):
                result.append(y_new)
        #создаём и заполняем списки для каждого примера
        all_result = [[0 for _ in range(10)] for _ in range(10)]
        for i in range(len(result)):
            #округляем каждый полученный индекс до ближайшего целого
            tmp_res = (result[i].round(2) * 100).round(1)
            #заполняем соответствующий примеру список единицей на указанной позиции
            all_result[i][int(tmp_res) - 1] = 1
        #return result
        return all_result

#тестовые примеры с шумом (максимально странные примеры в выборке, конечно...)
examples_with_noise = [[[1, 1, 1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, -1, 1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, -1, 1, 1, 1, -1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, 1, 1, 1, 1, 1, 1]],

                       [[1, 1, 1, 1, 1, -1, -1],
                        [1, 1, 1, -1, 1, 1, 1],
                        [1, 1, 1, -1, 1, -1, 1],
                        [1, 1, 1, -1, 1, 1, -1],
                        [1, 1, 1, -1, -1, 1, 1, ],
                        [1, 1, 1, -1, 1, 1, 1],
                        [1, 1, 1, 1, 1, 1, 1]],

                       [[1, 1, 1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, -1, -1],
                        [1, 1, -1, 1, 1, -1, 1],
                        [-1, -1, -1, -1, -1, -1, 1],
                        [1, 1, 1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, 1, 1, 1, 1, 1, 1]],

                       [[1, 1, -1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, 1, 1, 1, 1, -1, 1],
                        [1, 1, -1, -1, -1, 1, 1],
                        [1, -1, 1, 1, 1, -1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, 1, 1, 1, -1, 1, 1]],

                       [[1, 1, 1, 1, 1, 1, 1],
                        [1, -1, 1, -1, -1, 1, 1],
                        [1, 1, -1, 1, -1, 1, 1],
                        [1, -1, 1, 1, -1, -1, 1],
                        [1, -1, -1, -1, -1, 1, 1],
                        [1, 1, 1, 1, -1, -1, 1],
                        [1, 1, 1, 1, 1, -1, 1]],

                       [[1, -1, 1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, -1, -1, 1, 1, 1, -1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, 1, 1, 1, 1, -1, -1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, 1, 1, 1, 1, 1, 1]],

                       [[1, 1, 1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, 1, 1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, -1, -1],
                        [1, 1, 1, 1, 1, -1, 1],
                        [1, -1, 1, -1, -1, -1, 1],
                        [1, 1, 1, 1, 1, -1, 1]],

                       [[1, 1, -1, 1, 1, 1, 1, ],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, -1, 1, 1, -1, -1, 1],
                        [1, 1, 1, -1, 1, 1, 1],
                        [1, 1, 1, -1, 1, 1, 1],
                        [1, 1, 1, -1, 1, -1, 1],
                        [1, 1, 1, 1, -1, 1, 1]],

                       [[1, 1, 1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [-1, -1, 1, 1, 1, -1, 1],
                        [-1, 1, -1, -1, 1, 1, -1],
                        [1, -1, 1, 1, -1, -1, 1],
                        [1, -1, -1, -1, -1, -1, 1],
                        [1, 1, 1, 1, 1, 1, 1]],

                       [[1, 1, 1, 1, 1, 1, 1],
                        [1, -1, -1, -1, -1, 1, 1],
                        [1, -1, 1, 1, 1, -1, 1],
                        [1, -1, -1, -1, -1, 1, 1],
                        [1, 1, 1, 1, 1, -1, -1],
                        [1, -1, -1, 1, -1, 1, 1],
                        [1, 1, 1, 1, 1, 1, 1]]]

#"чистые" примеры
examples = [[[1, 1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, -1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, -1, 1, 1, 1, -1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, 1, 1]],

            [[1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, -1, 1, 1, 1],
             [1, 1, 1, -1, 1, 1, 1],
             [1, 1, 1, -1, 1, 1, 1],
             [1, 1, 1, -1, 1, 1, 1],
             [1, 1, 1, -1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1]],

            [[1, 1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, -1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, -1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, 1, 1]],

            [[1, 1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, -1, 1, 1, 1, -1, 1],
             [1, 1, -1, -1, -1, 1, 1],
             [1, -1, 1, 1, 1, -1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, 1, 1]],

            [[1, 1, 1, 1, 1, 1, 1],
             [1, 1, 1, -1, -1, 1, 1],
             [1, 1, -1, 1, -1, 1, 1],
             [1, -1, 1, 1, -1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, -1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1]],

            [[1, 1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, -1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, -1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, 1, 1]],

            [[1, 1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, -1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, -1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, 1, 1]],

            [[1, 1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, -1, 1],
             [1, 1, 1, 1, -1, 1, 1],
             [1, 1, 1, -1, 1, 1, 1],
             [1, 1, 1, -1, 1, 1, 1],
             [1, 1, 1, 1, 1, 1, 1]],

            [[1, 1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, -1, 1, 1, 1, -1, 1],
             [1, 1, -1, -1, -1, 1, 1],
             [1, -1, 1, 1, 1, -1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, 1, 1]],

            [[1, 1, 1, 1, 1, 1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, -1, 1, 1, 1, -1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, -1, 1],
             [1, -1, -1, -1, -1, -1, 1],
             [1, 1, 1, 1, 1, 1, 1]]]

network = Hamming_Network()
network.init_basic_mat(examples)
outputs_w1 = network.calculate_outputs_w1(examples_with_noise)
answers = network.calculate_outputs_w2(outputs_w1, examples_with_noise)
print(answers)
